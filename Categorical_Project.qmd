---
title: "MAT5317 Graduate Project"
author:
- Teng Li
- Zhize Lu
format: 
  html:
    toc: true
    fig_caption: true
server: shiny
bibliography: References.bib
link-citations: true
---

<style type="text/css">
.title, .author{text-align: center;}
body{font-size: 12pt;}
table{font-size: 12pt;}
h1{font-size: 14pt;}
h2{font-size: 12pt;}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(plotly)
library(tidyverse)
library(shiny)
library(caret)
source("CodeSpace.R")
```

# Introduction

Artificial Neural Networks (ANNs) are computational models inspired by the structure and functioning of the human brain, designed to mimic the way biological neural networks process information. These networks consist of interconnected nodes, or artificial neurons, organized into layers: an input layer, one or more hidden layers, and an output layer.

The fundamental building block of an artificial neuron is a mathematical function that takes weighted inputs, applies an activation function, and produces an output. Through a process of training, ANNs can be constructed to map input data to desired output, making them particularly adept at tasks such as classification, regression, and pattern recognition.

```{r}
#| panel: sidebar
textInput("vector_input", "Enter vector (comma-separated):", "2,3,1")
```

```{r}
#| panel: fill
plotlyOutput("plot1")
```

```{r}
#| context: server
selectedData<-reactive({
  inputs <- trimws(gsub("\\s", "", input$vector_input))
  as.numeric(strsplit(inputs, ",")[[1]])})

output$plot1 <- renderPlotly({
  inputs<-selectedData()
  
  layers<-lapply(inputs, function(x){seq(1:x)})
  
  for(i in 1:(length(layers)-1)){
    if(length(layers[[i+1]])==length(layers[[i]])){
      layers[[i+1]]<-layers[[i]]
    }
    else if(length(layers[[i+1]])>length(layers[[i]])){
      dis<-(max(layers[[i]])-min(layers[[i]]))/length(layers[[i]])
      original_vector <- layers[[i]]
      range_original <- c(min(original_vector)-dis, max(original_vector)+dis)
      layers[[i+1]] <- seq(range_original[1], range_original[2], length.out = length(layers[[i+1]]))
    }
    else if(length(layers[[i+1]])<length(layers[[i]])){
      dis<-(max(layers[[i]])-min(layers[[i]]))/length(layers[[i]])
      original_vector <- layers[[i]]
      range_original <- c(min(original_vector)+dis, max(original_vector)-dis)
      layers[[i+1]] <- seq(range_original[1], range_original[2], length.out = length(layers[[i+1]]))
    }
  }
  
  Neurons<-lapply(seq_along(layers), function(ind){matrix(c(rep(ind*10, length(layers[[ind]])), rep(1, length(layers[[ind]])), layers[[ind]]), ncol = 3)})
  Neurons<-map(Neurons, function(x){rbind(x, matrix(c(x[,1], rep(2, nrow(x)), x[,3]), ncol = 3))})
  Neurons<-map(Neurons, function(x){rbind(x, matrix(c(x[,1], rep(0, nrow(x)), x[,3]), ncol = 3))})
  
  dots<-do.call(rbind, Neurons)%>%data.frame()
  
  colnames(dots)<-c("X", "Y", "Z")
  
  # Create a 3D plot with dots and a line
  p<-dots%>%plot_ly()%>%
    add_trace(
      type = "scatter3d",
      mode = "markers",
      x = ~X,
      y = ~Y,
      z = ~Z,
      marker = list(size = 6, color = "#87CEFA", symbol = "cube")
    ) 
  
  vertices<-list()
  for(i in 1:(length(Neurons)-1)){
    for(j in 1:nrow(Neurons[[i]])){
      for(k in 1:nrow(Neurons[[i+1]])){
        vertices<-c(vertices, list(rbind(Neurons[[i]][j,], Neurons[[i+1]][k,])))
      }
    }
  }
  
  vertices<-map(vertices, function(x){df<-as.data.frame(x); colnames(df)<-c("X","Y","Z"); return(df)})
  
  for(i in 1:length(vertices)){
    p<-p%>%add_trace(
      type = "scatter3d",
      mode = "lines",
      x = vertices[[i]]$X,
      y = vertices[[i]]$Y,
      z = vertices[[i]]$Z,
      line = list(color = "#696969", width = 1)
    )
  }
  
  p<-p%>%layout(
    showlegend=FALSE,
    scene = list(aspectmode = "manual", aspectratio = list(x = 3, y = 1, z = 1),
                 xaxis = list(showgrid = FALSE, tickvals = NULL, zeroline = FALSE),
                 yaxis = list(showgrid = FALSE, tickvals = NULL, zeroline = FALSE),
                 zaxis = list(showgrid = FALSE, tickvals = NULL, zeroline = FALSE)),
    plot_bgcolor = "#232323",
    paper_bgcolor = "#232323")
  p
})
```

While there are many types of ANNs been developed, the original concept can go back to 1800s when theories of linear regression was established [@Hist]. From the basic feedforward neural network (FNN) that we demonstrated in this project, to the more complicated recurrent neural network (RNN), the types of neural networks depend on their architectures. For example, an FNN can have many hidden layers with neurons fully connect to each other, and information moves only one direction; it never goes backwards, whereas RNN can have bi-directional flow. Also radial-based networks use a radial function as the activation function. Here we mainly focus on FNN with a single hidden layer. Implementation of this simple neural network involves two key processes: forward propagation and backward propagation. In forward propagation, the input data is passed through the network, and the model produces an output. During backward propagation, the network learns by adjusting its internal parameters (weights and biases) based on the error between the predicted output and the actual target values. This iterative learning process allows the network to improve its performance over time.

Neural networks are especially powerful for solving complex problems, and their versatility has led to widespread use in various domains, including image and speech recognition, natural language processing, and many other applications.

Artificial neural networks have been developed to solve some of the typical problems that we can find in categorical analysis. For example, one can apply a logistic regression model to a binary data, but can also use neural network as a non-parametric model to achieve similar result. From our implementation section, one can see that ANNs can be used to solve a problem that is usually not possible by generalized linear models as the data cannot be linearly separated.

# Method

We aimed to understand how the neural network works and to implement it on simulated data. We then compared the result with the one generated by the logistic regression model. At the end we wanted to measure the performance between logistic regression and NN in this classical example.

As we previously mentioned, an ANN mainly consists of three major components: the inputs, the weights and biases, and the activation function. Together they form a neuron that takes the input information, process it and returns an output based on the activation as a form of threshold. The input forms the first layer of our neural network, often referred as the input layer. Similarly, the output layer consists of neurons that generate outputs. Any additional layers between the input and output layer are referred as the hidden layers. The input data are sent to the neurons in the next hidden layer. The neurons process the input as explained in the paper published by Catherine Higham and Desmond Higham [@DeepLearning].

Assuming our input data $x\in \mathbb{R}^2$, then the neurons take a linear transformation of the data and apply the activation function to it:

$$
\sigma(W^{[2]}x + b^{[2]})
$$
where $\sigma(z)$ is the activation function and $W^{[2]}$ and $b^{[2]}$ are the weights and bias matrix for the second layer respectively. Since one can have many neurons in the hidden layer, assuming $n_l$ number of neurons in the current layer and $n_{l-1}$ number of neurons at the previous layer, then $W$ is an $n_l \times n_{l-1}$ matrix, and $b$ is a $n_{l-1} \times 1$ vector. Therefore assuming we have two neurons in the second layer, then we have the output as:

\begin{gather*}
\sigma(W^{[2]}x + b^{[2]}) \in \mathbb{R}^{2 \times 1}\\
W^{[2]} \in \mathbb{R}^{2 \times 2}, ~ b^{[2]} \in \mathbb{R}^{2 \times 1}
\end{gather*}

Similarly, if one has a third layer with three neurons, the output becomes:

\begin{gather*}
\sigma(W^{[3]} \sigma(W^{[2]}x + b^{[2]}) + b^{[3]}) \in \mathbb{R}^{3 \times 1}\\
W^{[3]} \in \mathbb{R}^{3 \times 2}, ~ b^{[3]} \in \mathbb{R}^{3 \times 1}
\end{gather*}

Therefore, the output is actually a function of all the weights and biases matrices. We can define the output of neurons in each layer as the following:

\begin{gather*}
a^{[1]} \coloneqq x \in \mathbb{R}^{n_1}\\
a^{[l]} \coloneqq \sigma(W^{[l]} \cdot a^{[l-1]} + b^{[l]}) \in \mathbb{R}^{n_l},~ \forall l=2,3,...,L
\end{gather*}

If we further defined the loss function:

$$
Cost=\frac{1}{2N}\sum_{i=1}^{N}(y_i - a_i^{[L]})^2
$$
then the goal is to minimize the loss with respect to the weights and biases. Finding the optimal solution of the weights and biases is generally referred as model learning. 

To find the optimal solution required computational methods. The classical approach is to use gradient descent:

suppose we have the parameter of interest as $p\in \mathbb{R}^d$, then for $t={0,1,2...}$ we have:
$$
p_{t+1} = p_t -\eta \cdot \nabla Cost(p_t)
$$
where $\eta$ is often referred as the learning rate. 

One of the biggest advantages of gradient descent is that one is not constrained by dimensionality. However the algorithm can be stuck in a saddle point. In addition, one must know how to calculate $\nabla Cost(p_t)$ and a good choice of $\eta$ is often required to avoid divergence of iterations. One can see in the Simulation section that our choice of $\eta$ was 0.9. Finding the optimal learning rate is beyond our scope, however the article[@LearningRate] explained that "Generally, a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights. A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train.".

The calculation of the derivatives of the lost function with respect to weights and biases is not trivial. One often has to work on cumbersome vector calculus[@MatrixCalc]. Given the mean squared error loss and the sigmoid activation function $\sigma(z):= \frac{1}{1+e^{-z}}$, the authors of the paper[@DeepLearning] have provided the solutions:

Define $\delta^{[l]}:=\frac{\partial C}{\partial z_i^{[l]}},~ 1\leq i \leq n_l,~2\leq l \leq L$, and $D^{[l]}:=diag(\sigma^{'}(z_i^{[l]})) \in \mathbb{R}^{n_l \times n_l}$ then

\begin{align}
\delta^{[L]}:=D^{[L]} \cdot (a^{[L]}-y)  \tag{eq1}\\
\delta^{[l]}:=D^{[l]} \cdot (W^{[l+1]})^T \cdot \delta^{[l+1]}  \tag{eq2}\\
\frac{\partial C}{\partial w_{ij}^{[l]}}:=\delta_i^{[l]}\cdot a_j^{[l-1]}  \tag{eq3}\\
\frac{\partial C}{\partial b_{i}^{[l]}}:=\delta_i^{[l]}  \tag{eq4}\\
\end{align}

As one may have noticed, we first calculate $a^L$ from a forward pass, known as the forward propagation, and then calculate $\delta^{[L]}$ first and then $\delta^{[l]},~l=L-1, L-2,....$ as well as the partial derivatives from a backward pass, known as the backward propagation.

# Simulation

We used a simple Uniform random number generating mechanism to generate nonlinear categorical data based on two covariates [@Data]. Based on the mathematical expressions explained in Section Method, we built our algorithm according to an existing template provided by [@Algo]. The following Figure 1 showed the relationship between the response Y and the covariates X1 and X2:

```{r}
set.seed(103)
circulo <- function(x, R, centroX=0, centroY=0){
r = R * sqrt(runif(x))
theta = runif(x) * 2 * pi
x1 = centroX + r * cos(theta)
x2 = centroY + r * sin(theta)

z = data.frame(x1 = x1, x2 = x2)
return(z)
}

datos1 <- circulo(600,0.5)
datos2 <- circulo(600,1.5)
datos1$y <- 1
datos2$y <- 0
datos <- rbind(datos1,datos2)
rm(datos1,datos2, circulo)
rws<-sample(1:nrow(datos))
datos <-datos[rws,]

ggplot(datos,aes(x1,x2, col = as.factor(y))) + geom_point()+labs(title="Figure 1: Original Response and Covariates")
```

As one can see, the response could not be linearly separated into different clusters.

For comparison, we first employed a simple logistic regression, a fundamental statistical model for binary classification, to analyze the dataset we simulated.

In logistic regression, the probability of the target variable $Y$ being in a particular class is modeled as a logistic function of a linear combination of the explanatory variables $(X_1,X_2,...,X_n)$. Our simulated data set only included $(X_1,X_2)$. 

The logistic function, often called the sigmoid function, is defined as: 
$\sigma(x)=\frac{1}{1+e^{-x}}$

For our binary classification, the logistic regression model is expressed as:
$P(Y=1|X)=\frac{1}{1+e^{-(\beta_0+\beta_1X_1+\beta_2X_2)}}$
, where $P(Y=1|X)$ is the probability that the target variable $Y$ is in class 1 given the predictor $X=(X_1,X_2)$.$\beta_0,\beta_1,\beta_2$ are the coefficients of the model.

The logistic regression model was implemented using R's 'glm' function with a binomial family. The function estimated the coefficients of the model using maximum likelihood estimation. We trained the model on the training set, which includes 800 observations.

```{r}
train_data <- datos[1:800, ]
test_data <- datos[801:1200, ]

logistic_model <- glm(y ~ x1 + x2, data = train_data, family = binomial(link = "logit"))

p <- predict(logistic_model, newdata = test_data, type = "response")
glm_pred <- ifelse(p > 0.5, 1, 0)

df_glm_pred<-data.frame(x1=test_data[,"x1"], x2=test_data[,"x2"], y_pred=as.numeric(glm_pred), y_true=test_data[,"y"])%>%
  gather("Type", "Val", 3:4)

ggplot(df_glm_pred, aes(x1, x2, col=as.factor(Val))) +
        geom_point() +
        facet_wrap(~Type,scales = 'free',labeller = label_parsed) + theme_bw() +
        labs(title="Figure 2: Logistic Regression Model Prediction",
          x = "X1",
             y = "X2")
```

We then performed the neural network prediction on the same data set. 

```{r}
test<-NeuralNet$new(data=train_data, hidden_neurons=40, num_iteration=40000, learning_rate=0.95)

plot_ly(y=test$output$cost_hist, type = "scatter", mode="lines",line = list(width = 1))%>%
  layout(title="Figure 3: Loss History of Feedforward Neural Network",
         xaxis=list(title="Iteration"),
         yaxis=list(title="Mean Squared Error"))
```

```{r}
y_pred<-test$makePrediction(testdata=test_data, 40)

df_pred<-data.frame(x1=test_data$x1, x2=test_data$x2, y_pred=as.numeric(y_pred), y_true=test_data$y)%>%
  gather("Type", "Val", 3:4)

ggplot(df_pred, aes(x1, x2, col=as.factor(Val))) +
        geom_point() +
        facet_wrap(~Type,scales = 'free',labeller = label_parsed) + theme_bw() +
        labs(title="Figure 4: Feedforward Neural Network Model Prediction",
             x = "X1",
             y = "X2")
```

From Figure 2 comparing the predict data points with the original data points, we saw that due to the linear nature of the logistic regression model, it was inadequate for capturing the circular patterns of the data. On the other hand, one can see from Figure 4 that the NN model has no problem to classify complex data with circular distribution.

To better describe the performance of the model classification, we used the confusion matrix. The confusion matrix can be represented as:
$$Confusion\ Matrix=\begin{bmatrix}TrueNegative & FalsePositive\\ FalseNegative & TruePositive\end{bmatrix}$$

True Positives (TP): Correctly predicted positive observations.
True Negatives (TN): Correctly predicted negative observations.
False Positives (FP): Incorrectly predicted positive observations (Type I error).
False Negatives (FN): Incorrectly predicted negative observations (Type II error).

With the confusion matrix, we examed the following measures of classificaiton:
$$Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$$
$$Recall=\frac{TP}{TP+FN}$$
$$Precision=\frac{TP}{TP+FP}$$
$$F1\ Score=2\times\frac{Precision\times Recall}{Precision+Recall}$$

We chose to compare the Accuracy and the F1-Score of both logistic regression model and NN model. The accuracy measureed the overall correctness of the model, the proportion of total true predictions to the total number of cases. The F1-Score balanced the trade-off between precision and recall.
```{r}
cm_logistic <- confusionMatrix(as.factor(glm_pred), as.factor(test_data$y))
table_logistic <- cm_logistic$table

cm_nn <- confusionMatrix(as.factor(y_pred), as.factor(datos[801:1200,"y"]))
table_nn <- cm_nn$table

print("Confusion Matrix - Logistic Regression:")
print(table_logistic)

print("Confusion Matrix - Neural Network:")
print(table_nn)
```

```{r}
accuracy_logistic <- cm_logistic$overall['Accuracy']
precision_logistic <- cm_logistic$byClass['Precision']
recall_logistic <- cm_logistic$byClass['Recall']
f1_score_logistic <- 2 * (precision_logistic * recall_logistic) / (precision_logistic + recall_logistic)

accuracy_nn <- cm_nn$overall['Accuracy']
precision_nn <- cm_nn$byClass['Precision']
recall_nn <- cm_nn$byClass['Recall']
f1_score_nn <- 2 * (precision_nn * recall_nn) / (precision_nn + recall_nn)

comparison_table <- data.frame(
  Model = c("Logistic Regression", "Neural Network"),
  Accuracy = c(accuracy_logistic, accuracy_nn),
  F1_Score = c(f1_score_logistic, f1_score_nn)
)

print(comparison_table)
```

The table above showed that the Neural Network model substantially outperformed the Logistic Regression model on our simulated data set. The higher values in both accuracy and F1 Score for the Neural Network suggested that it was more capable of handling the complexities of the data set, particularly its non-linear patterns.

# Conclusion

We found that when the data could not be linearly separated, the logistic model failed to make accurate prediction, whereas a simple feedforward neural network outperformed the logistic model. However one may try fitting a different logistic model such as having a multiplicative relationship between the covariates instead of an additive correlation. Our findings do not conclude in general that neural networks always perform better, because the result depends on many factors such as the choice of activation functions and learning rate. The architecture of the neural network plays a crucial part in its success. Therefore our project aimed to give an introductory exploration of the difference between logistic regression and neural network, rather than having a rigorous comparison of the performance of these two approaches. The modern neural network techniques have given new approaches in analyzing categorical data, and we hope our project has demonstrated such possibility. 








